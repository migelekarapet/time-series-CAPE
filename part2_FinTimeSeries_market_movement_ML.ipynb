{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import time \n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "#import math\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "\n",
    "from pandas import concat\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_tree\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard data loading, cleaning and missing values filling steps are below, followed by data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>P</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>GS10</th>\n",
       "      <th>Price</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871,01</td>\n",
       "      <td>4,44</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,46</td>\n",
       "      <td>1871,04</td>\n",
       "      <td>5,32</td>\n",
       "      <td>91,37</td>\n",
       "      <td>5,35</td>\n",
       "      <td>8,23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871,02</td>\n",
       "      <td>4,50</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,84</td>\n",
       "      <td>1871,13</td>\n",
       "      <td>5,32</td>\n",
       "      <td>89,86</td>\n",
       "      <td>5,19</td>\n",
       "      <td>7,99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871,03</td>\n",
       "      <td>4,61</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>13,03</td>\n",
       "      <td>1871,21</td>\n",
       "      <td>5,33</td>\n",
       "      <td>90,71</td>\n",
       "      <td>5,12</td>\n",
       "      <td>7,87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1871,04</td>\n",
       "      <td>4,74</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,56</td>\n",
       "      <td>1871,29</td>\n",
       "      <td>5,33</td>\n",
       "      <td>96,81</td>\n",
       "      <td>5,31</td>\n",
       "      <td>8,17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1871,05</td>\n",
       "      <td>4,86</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,27</td>\n",
       "      <td>1871,37</td>\n",
       "      <td>5,33</td>\n",
       "      <td>101,57</td>\n",
       "      <td>5,43</td>\n",
       "      <td>8,36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1871,06</td>\n",
       "      <td>4,82</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,08</td>\n",
       "      <td>1871,46</td>\n",
       "      <td>5,34</td>\n",
       "      <td>102,32</td>\n",
       "      <td>5,52</td>\n",
       "      <td>8,49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871,07</td>\n",
       "      <td>4,73</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,08</td>\n",
       "      <td>1871,54</td>\n",
       "      <td>5,34</td>\n",
       "      <td>100,41</td>\n",
       "      <td>5,52</td>\n",
       "      <td>8,49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871,08</td>\n",
       "      <td>4,79</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>11,89</td>\n",
       "      <td>1871,62</td>\n",
       "      <td>5,34</td>\n",
       "      <td>103,31</td>\n",
       "      <td>5,61</td>\n",
       "      <td>8,63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871,09</td>\n",
       "      <td>4,84</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,18</td>\n",
       "      <td>1871,71</td>\n",
       "      <td>5,35</td>\n",
       "      <td>101,94</td>\n",
       "      <td>5,48</td>\n",
       "      <td>8,42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1871,1</td>\n",
       "      <td>4,59</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,37</td>\n",
       "      <td>1871,79</td>\n",
       "      <td>5,35</td>\n",
       "      <td>95,19</td>\n",
       "      <td>5,39</td>\n",
       "      <td>8,30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1871,11</td>\n",
       "      <td>4,64</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,37</td>\n",
       "      <td>1871,87</td>\n",
       "      <td>5,35</td>\n",
       "      <td>96,22</td>\n",
       "      <td>5,39</td>\n",
       "      <td>8,30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1871,12</td>\n",
       "      <td>4,74</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,65</td>\n",
       "      <td>1871,96</td>\n",
       "      <td>5,36</td>\n",
       "      <td>96,08</td>\n",
       "      <td>5,27</td>\n",
       "      <td>8,11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1872,01</td>\n",
       "      <td>4,86</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,65</td>\n",
       "      <td>1872,04</td>\n",
       "      <td>5,36</td>\n",
       "      <td>98,51</td>\n",
       "      <td>5,34</td>\n",
       "      <td>8,16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1872,02</td>\n",
       "      <td>4,88</td>\n",
       "      <td>0,27</td>\n",
       "      <td>0,41</td>\n",
       "      <td>12,65</td>\n",
       "      <td>1872,12</td>\n",
       "      <td>5,38</td>\n",
       "      <td>98,92</td>\n",
       "      <td>5,41</td>\n",
       "      <td>8,21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1872,03</td>\n",
       "      <td>5,04</td>\n",
       "      <td>0,27</td>\n",
       "      <td>0,41</td>\n",
       "      <td>12,84</td>\n",
       "      <td>1872,21</td>\n",
       "      <td>5,40</td>\n",
       "      <td>100,65</td>\n",
       "      <td>5,39</td>\n",
       "      <td>8,14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date     P     D     E    CPI Fraction  GS10   Price Dividend Earnings\n",
       "0   1871,01  4,44  0,26  0,40  12,46  1871,04  5,32   91,37     5,35     8,23\n",
       "1   1871,02  4,50  0,26  0,40  12,84  1871,13  5,32   89,86     5,19     7,99\n",
       "2   1871,03  4,61  0,26  0,40  13,03  1871,21  5,33   90,71     5,12     7,87\n",
       "3   1871,04  4,74  0,26  0,40  12,56  1871,29  5,33   96,81     5,31     8,17\n",
       "4   1871,05  4,86  0,26  0,40  12,27  1871,37  5,33  101,57     5,43     8,36\n",
       "5   1871,06  4,82  0,26  0,40  12,08  1871,46  5,34  102,32     5,52     8,49\n",
       "6   1871,07  4,73  0,26  0,40  12,08  1871,54  5,34  100,41     5,52     8,49\n",
       "7   1871,08  4,79  0,26  0,40  11,89  1871,62  5,34  103,31     5,61     8,63\n",
       "8   1871,09  4,84  0,26  0,40  12,18  1871,71  5,35  101,94     5,48     8,42\n",
       "9    1871,1  4,59  0,26  0,40  12,37  1871,79  5,35   95,19     5,39     8,30\n",
       "10  1871,11  4,64  0,26  0,40  12,37  1871,87  5,35   96,22     5,39     8,30\n",
       "11  1871,12  4,74  0,26  0,40  12,65  1871,96  5,36   96,08     5,27     8,11\n",
       "12  1872,01  4,86  0,26  0,40  12,65  1872,04  5,36   98,51     5,34     8,16\n",
       "13  1872,02  4,88  0,27  0,41  12,65  1872,12  5,38   98,92     5,41     8,21\n",
       "14  1872,03  5,04  0,27  0,41  12,84  1872,21  5,40  100,65     5,39     8,14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#used as temporary solution - if there's time must fill missing values in a more meaningful way\n",
    "df = pd.read_csv('data.csv').fillna(method='ffill')\n",
    "df.head(15)\n",
    "\n",
    "# add drop dups\n",
    "df.drop_duplicates(keep=False,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date2(date):\n",
    "    if date.endswith(\",1\"):\n",
    "        date += \"0\"\n",
    "    return datetime.strptime(date, \"%Y,%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def parse_date(date):\\n    year, month = date.split(\",\", maxsplit=1)\\n    if month == \"1\":\\n        month = \"10\"\\n    return datetime(int(year), int(month), 1)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def parse_date(date):\n",
    "    year, month = date.split(\",\", maxsplit=1)\n",
    "    if month == \"1\":\n",
    "        month = \"10\"\n",
    "    return datetime(int(year), int(month), 1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dared to drop Fractions column as the initial research shown it is not adding much value\n",
    "# e.g. https://datahub.io/core/s-and-p-500\n",
    "df.drop(\"Fraction\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].apply(parse_date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>P</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>CPI</th>\n",
       "      <th>GS10</th>\n",
       "      <th>Price</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871-01-01</td>\n",
       "      <td>4,44</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,46</td>\n",
       "      <td>5,32</td>\n",
       "      <td>91,37</td>\n",
       "      <td>5,35</td>\n",
       "      <td>8,23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871-02-01</td>\n",
       "      <td>4,50</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,84</td>\n",
       "      <td>5,32</td>\n",
       "      <td>89,86</td>\n",
       "      <td>5,19</td>\n",
       "      <td>7,99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871-03-01</td>\n",
       "      <td>4,61</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>13,03</td>\n",
       "      <td>5,33</td>\n",
       "      <td>90,71</td>\n",
       "      <td>5,12</td>\n",
       "      <td>7,87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1871-04-01</td>\n",
       "      <td>4,74</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,56</td>\n",
       "      <td>5,33</td>\n",
       "      <td>96,81</td>\n",
       "      <td>5,31</td>\n",
       "      <td>8,17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1871-05-01</td>\n",
       "      <td>4,86</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,27</td>\n",
       "      <td>5,33</td>\n",
       "      <td>101,57</td>\n",
       "      <td>5,43</td>\n",
       "      <td>8,36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1871-06-01</td>\n",
       "      <td>4,82</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,08</td>\n",
       "      <td>5,34</td>\n",
       "      <td>102,32</td>\n",
       "      <td>5,52</td>\n",
       "      <td>8,49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871-07-01</td>\n",
       "      <td>4,73</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,08</td>\n",
       "      <td>5,34</td>\n",
       "      <td>100,41</td>\n",
       "      <td>5,52</td>\n",
       "      <td>8,49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871-08-01</td>\n",
       "      <td>4,79</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>11,89</td>\n",
       "      <td>5,34</td>\n",
       "      <td>103,31</td>\n",
       "      <td>5,61</td>\n",
       "      <td>8,63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871-09-01</td>\n",
       "      <td>4,84</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,18</td>\n",
       "      <td>5,35</td>\n",
       "      <td>101,94</td>\n",
       "      <td>5,48</td>\n",
       "      <td>8,42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1871-10-01</td>\n",
       "      <td>4,59</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,37</td>\n",
       "      <td>5,35</td>\n",
       "      <td>95,19</td>\n",
       "      <td>5,39</td>\n",
       "      <td>8,30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1871-11-01</td>\n",
       "      <td>4,64</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,37</td>\n",
       "      <td>5,35</td>\n",
       "      <td>96,22</td>\n",
       "      <td>5,39</td>\n",
       "      <td>8,30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1871-12-01</td>\n",
       "      <td>4,74</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,65</td>\n",
       "      <td>5,36</td>\n",
       "      <td>96,08</td>\n",
       "      <td>5,27</td>\n",
       "      <td>8,11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1872-01-01</td>\n",
       "      <td>4,86</td>\n",
       "      <td>0,26</td>\n",
       "      <td>0,40</td>\n",
       "      <td>12,65</td>\n",
       "      <td>5,36</td>\n",
       "      <td>98,51</td>\n",
       "      <td>5,34</td>\n",
       "      <td>8,16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1872-02-01</td>\n",
       "      <td>4,88</td>\n",
       "      <td>0,27</td>\n",
       "      <td>0,41</td>\n",
       "      <td>12,65</td>\n",
       "      <td>5,38</td>\n",
       "      <td>98,92</td>\n",
       "      <td>5,41</td>\n",
       "      <td>8,21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1872-03-01</td>\n",
       "      <td>5,04</td>\n",
       "      <td>0,27</td>\n",
       "      <td>0,41</td>\n",
       "      <td>12,84</td>\n",
       "      <td>5,40</td>\n",
       "      <td>100,65</td>\n",
       "      <td>5,39</td>\n",
       "      <td>8,14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1872-04-01</td>\n",
       "      <td>5,18</td>\n",
       "      <td>0,27</td>\n",
       "      <td>0,41</td>\n",
       "      <td>13,13</td>\n",
       "      <td>5,42</td>\n",
       "      <td>101,19</td>\n",
       "      <td>5,34</td>\n",
       "      <td>8,01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1872-05-01</td>\n",
       "      <td>5,18</td>\n",
       "      <td>0,28</td>\n",
       "      <td>0,41</td>\n",
       "      <td>13,13</td>\n",
       "      <td>5,43</td>\n",
       "      <td>101,19</td>\n",
       "      <td>5,41</td>\n",
       "      <td>8,06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1872-06-01</td>\n",
       "      <td>5,13</td>\n",
       "      <td>0,28</td>\n",
       "      <td>0,42</td>\n",
       "      <td>13,03</td>\n",
       "      <td>5,45</td>\n",
       "      <td>100,95</td>\n",
       "      <td>5,51</td>\n",
       "      <td>8,17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1872-07-01</td>\n",
       "      <td>5,10</td>\n",
       "      <td>0,28</td>\n",
       "      <td>0,42</td>\n",
       "      <td>12,84</td>\n",
       "      <td>5,47</td>\n",
       "      <td>101,84</td>\n",
       "      <td>5,66</td>\n",
       "      <td>8,34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1872-08-01</td>\n",
       "      <td>5,04</td>\n",
       "      <td>0,29</td>\n",
       "      <td>0,42</td>\n",
       "      <td>12,94</td>\n",
       "      <td>5,49</td>\n",
       "      <td>99,91</td>\n",
       "      <td>5,68</td>\n",
       "      <td>8,33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1872-09-01</td>\n",
       "      <td>4,95</td>\n",
       "      <td>0,29</td>\n",
       "      <td>0,42</td>\n",
       "      <td>13,03</td>\n",
       "      <td>5,51</td>\n",
       "      <td>97,41</td>\n",
       "      <td>5,71</td>\n",
       "      <td>8,31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1872-10-01</td>\n",
       "      <td>4,97</td>\n",
       "      <td>0,29</td>\n",
       "      <td>0,43</td>\n",
       "      <td>12,75</td>\n",
       "      <td>5,53</td>\n",
       "      <td>99,99</td>\n",
       "      <td>5,90</td>\n",
       "      <td>8,55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1872-11-01</td>\n",
       "      <td>4,95</td>\n",
       "      <td>0,30</td>\n",
       "      <td>0,43</td>\n",
       "      <td>13,13</td>\n",
       "      <td>5,54</td>\n",
       "      <td>96,70</td>\n",
       "      <td>5,80</td>\n",
       "      <td>8,35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1872-12-01</td>\n",
       "      <td>5,07</td>\n",
       "      <td>0,30</td>\n",
       "      <td>0,43</td>\n",
       "      <td>12,94</td>\n",
       "      <td>5,56</td>\n",
       "      <td>100,50</td>\n",
       "      <td>5,95</td>\n",
       "      <td>8,52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1873-01-01</td>\n",
       "      <td>5,11</td>\n",
       "      <td>0,30</td>\n",
       "      <td>0,43</td>\n",
       "      <td>12,94</td>\n",
       "      <td>5,58</td>\n",
       "      <td>101,29</td>\n",
       "      <td>6,00</td>\n",
       "      <td>8,57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     P     D     E    CPI  GS10   Price Dividend Earnings\n",
       "0  1871-01-01  4,44  0,26  0,40  12,46  5,32   91,37     5,35     8,23\n",
       "1  1871-02-01  4,50  0,26  0,40  12,84  5,32   89,86     5,19     7,99\n",
       "2  1871-03-01  4,61  0,26  0,40  13,03  5,33   90,71     5,12     7,87\n",
       "3  1871-04-01  4,74  0,26  0,40  12,56  5,33   96,81     5,31     8,17\n",
       "4  1871-05-01  4,86  0,26  0,40  12,27  5,33  101,57     5,43     8,36\n",
       "5  1871-06-01  4,82  0,26  0,40  12,08  5,34  102,32     5,52     8,49\n",
       "6  1871-07-01  4,73  0,26  0,40  12,08  5,34  100,41     5,52     8,49\n",
       "7  1871-08-01  4,79  0,26  0,40  11,89  5,34  103,31     5,61     8,63\n",
       "8  1871-09-01  4,84  0,26  0,40  12,18  5,35  101,94     5,48     8,42\n",
       "9  1871-10-01  4,59  0,26  0,40  12,37  5,35   95,19     5,39     8,30\n",
       "10 1871-11-01  4,64  0,26  0,40  12,37  5,35   96,22     5,39     8,30\n",
       "11 1871-12-01  4,74  0,26  0,40  12,65  5,36   96,08     5,27     8,11\n",
       "12 1872-01-01  4,86  0,26  0,40  12,65  5,36   98,51     5,34     8,16\n",
       "13 1872-02-01  4,88  0,27  0,41  12,65  5,38   98,92     5,41     8,21\n",
       "14 1872-03-01  5,04  0,27  0,41  12,84  5,40  100,65     5,39     8,14\n",
       "15 1872-04-01  5,18  0,27  0,41  13,13  5,42  101,19     5,34     8,01\n",
       "16 1872-05-01  5,18  0,28  0,41  13,13  5,43  101,19     5,41     8,06\n",
       "17 1872-06-01  5,13  0,28  0,42  13,03  5,45  100,95     5,51     8,17\n",
       "18 1872-07-01  5,10  0,28  0,42  12,84  5,47  101,84     5,66     8,34\n",
       "19 1872-08-01  5,04  0,29  0,42  12,94  5,49   99,91     5,68     8,33\n",
       "20 1872-09-01  4,95  0,29  0,42  13,03  5,51   97,41     5,71     8,31\n",
       "21 1872-10-01  4,97  0,29  0,43  12,75  5,53   99,99     5,90     8,55\n",
       "22 1872-11-01  4,95  0,30  0,43  13,13  5,54   96,70     5,80     8,35\n",
       "23 1872-12-01  5,07  0,30  0,43  12,94  5,56  100,50     5,95     8,52\n",
       "24 1873-01-01  5,11  0,30  0,43  12,94  5,58  101,29     6,00     8,57"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>    1783\n",
       "Name: P, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# those are dtype (str) type of objects and need to be converted\n",
    "df['P'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining auxiliary cleaning function\n",
    "def clean_column(x):\n",
    "    \"\"\" the values are str objects containing ',' those have been replaced with '.'\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        return(x.replace(',', '.'))\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['P', 'D', 'E', 'CPI', 'GS10', 'Price', 'Dividend', 'Earnings'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['P'] = df['P'].apply(clean_column).astype('float')\n",
    "cols = df.columns[df.dtypes.eq('object')]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting all 'object' type of columns in one shot\n",
    "# there might be even shorter way - skipping for loop - investigate later if time permits\n",
    "cols = df.columns[df.dtypes.eq('object')]\n",
    "\n",
    "for column in cols:\n",
    "    df[column] = df[column].apply(clean_column).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>CPI</th>\n",
       "      <th>GS10</th>\n",
       "      <th>Price</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1871-01-01</th>\n",
       "      <td>4.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.46</td>\n",
       "      <td>5.32</td>\n",
       "      <td>91.37</td>\n",
       "      <td>5.35</td>\n",
       "      <td>8.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-02-01</th>\n",
       "      <td>4.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.84</td>\n",
       "      <td>5.32</td>\n",
       "      <td>89.86</td>\n",
       "      <td>5.19</td>\n",
       "      <td>7.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-03-01</th>\n",
       "      <td>4.61</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>13.03</td>\n",
       "      <td>5.33</td>\n",
       "      <td>90.71</td>\n",
       "      <td>5.12</td>\n",
       "      <td>7.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-04-01</th>\n",
       "      <td>4.74</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.56</td>\n",
       "      <td>5.33</td>\n",
       "      <td>96.81</td>\n",
       "      <td>5.31</td>\n",
       "      <td>8.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-05-01</th>\n",
       "      <td>4.86</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.27</td>\n",
       "      <td>5.33</td>\n",
       "      <td>101.57</td>\n",
       "      <td>5.43</td>\n",
       "      <td>8.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-06-01</th>\n",
       "      <td>4.82</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.08</td>\n",
       "      <td>5.34</td>\n",
       "      <td>102.32</td>\n",
       "      <td>5.52</td>\n",
       "      <td>8.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-07-01</th>\n",
       "      <td>4.73</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.08</td>\n",
       "      <td>5.34</td>\n",
       "      <td>100.41</td>\n",
       "      <td>5.52</td>\n",
       "      <td>8.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-08-01</th>\n",
       "      <td>4.79</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>11.89</td>\n",
       "      <td>5.34</td>\n",
       "      <td>103.31</td>\n",
       "      <td>5.61</td>\n",
       "      <td>8.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-09-01</th>\n",
       "      <td>4.84</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.18</td>\n",
       "      <td>5.35</td>\n",
       "      <td>101.94</td>\n",
       "      <td>5.48</td>\n",
       "      <td>8.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-10-01</th>\n",
       "      <td>4.59</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.37</td>\n",
       "      <td>5.35</td>\n",
       "      <td>95.19</td>\n",
       "      <td>5.39</td>\n",
       "      <td>8.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               P     D    E    CPI  GS10   Price  Dividend  Earnings\n",
       "Date                                                                \n",
       "1871-01-01  4.44  0.26  0.4  12.46  5.32   91.37      5.35      8.23\n",
       "1871-02-01  4.50  0.26  0.4  12.84  5.32   89.86      5.19      7.99\n",
       "1871-03-01  4.61  0.26  0.4  13.03  5.33   90.71      5.12      7.87\n",
       "1871-04-01  4.74  0.26  0.4  12.56  5.33   96.81      5.31      8.17\n",
       "1871-05-01  4.86  0.26  0.4  12.27  5.33  101.57      5.43      8.36\n",
       "1871-06-01  4.82  0.26  0.4  12.08  5.34  102.32      5.52      8.49\n",
       "1871-07-01  4.73  0.26  0.4  12.08  5.34  100.41      5.52      8.49\n",
       "1871-08-01  4.79  0.26  0.4  11.89  5.34  103.31      5.61      8.63\n",
       "1871-09-01  4.84  0.26  0.4  12.18  5.35  101.94      5.48      8.42\n",
       "1871-10-01  4.59  0.26  0.4  12.37  5.35   95.19      5.39      8.30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file to have a copy\n",
    "df.to_csv('fotmatted_series.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why log returns and moving averages are used: \n",
    "we should normalize tha data and draw autocorrelation log returns and lagged log returns and lagged volatilities while looking up for patterns.\n",
    "Log returns are considered a standard transforamtional practice.\n",
    "Rest of features are creating new smoothed features. We'd plot some of them, draw inferences from correlation matrix and use them in our ML algorithms below (e.g. moving average can be used as a source of new information when modeling a time series forecast as a supervised learning problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['pct_change'] = df['P'].pct_change().fillna(method='bfill')\n",
    "df['log_ret_1_mth'] = np.log(df['P'] / df['P'].shift(1)).fillna(method='bfill')\n",
    "\n",
    "df['Log_Ret_2_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=2).sum()\n",
    "df['Log_Ret_3_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=3).sum()\n",
    "df['Log_Ret_4_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=4).sum()\n",
    "df['Log_Ret_8_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=8).sum()\n",
    "df['Log_Ret_12_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=12).sum()\n",
    "df['Log_Ret_48_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=48).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['volat_2_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=2).std()*np.sqrt(2)\n",
    "df['volat_3_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=3).std()*np.sqrt(3)\n",
    "df['volat_4_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=4).std()*np.sqrt(4)\n",
    "df['volat_8_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=8).std()*np.sqrt(8)\n",
    "df['volat_12_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=12).std()*np.sqrt(12)\n",
    "df['volat_48_mth']=pd.Series(df['log_ret_1_mth']).rolling(window=48).std()*np.sqrt(48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPE ratio engineering\n",
    "Also, this dataset was created as a part of larger study by Robert J. Shiller, [2]  who invented a precitvive index called CAPE (Cyclically adjusted price-to-earnings ratio)  which is said have a good predictive characteristics. \n",
    "Let's try to engineer it from the data we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also construct 12 months and 10 year moving average features along the way. Those can be used as a source of new information when modeling a time series forecast as a supervised learning problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECONSIDER - we have those already - \n",
    "# to catch for so-called trends (moving averages) let's engineer rolling mean \n",
    "df['rolling_Earnings_mean12'] = df['Earnings'].rolling(window=12).mean()\n",
    "df['rolling_Earnings_mean120'] = df['Earnings'].rolling(window=120).mean()\n",
    "df['CAPE10'] = (df['Price']/df['rolling_Earnings_mean120'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['2008']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General testing of results in 4-5 random years (1882, 1929, 2008-2009) revealed that the calculation is quite matching the original calculations in [2]. They seemed to base calculations on yearly data though, while we have monthly data. I decided to consider results acceptable to some degree, and not to invest time in finding the exact conversion techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a direction of change feature. This will be used as out Y variable in coming classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data: Up (Down) if the the 1 month logarithmic return increased (decreased)\n",
    "# shift index axis by a period in negative direction \n",
    " \n",
    "df['dir']=np.where(df['log_ret_1_mth'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows, Columns:\n",
      "(1783, 26)\n",
      "Columns:\n",
      "Index(['P', 'D', 'E', 'CPI', 'GS10', 'Price', 'Dividend', 'Earnings',\n",
      "       'pct_change', 'log_ret_1_mth', 'Log_Ret_2_mth', 'Log_Ret_3_mth',\n",
      "       'Log_Ret_4_mth', 'Log_Ret_8_mth', 'Log_Ret_12_mth', 'Log_Ret_48_mth',\n",
      "       'volat_2_mth', 'volat_3_mth', 'volat_4_mth', 'volat_8_mth',\n",
      "       'volat_12_mth', 'volat_48_mth', 'rolling_Earnings_mean12',\n",
      "       'rolling_Earnings_mean120', 'CAPE10', 'dir'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# list rows and columns\n",
    "print(\"Rows, Columns:\")\n",
    "print(df.shape)\n",
    "\n",
    "# list columns\n",
    "print(\"Columns:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backfilling with same values, CAPE, volatilities, rolling earning means \n",
    "# should not impact much since those are NaN's primarily in 1870's\n",
    "df = df.iloc[:].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1783 entries, 1871-01-01 to 2019-07-01\n",
      "Data columns (total 26 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   P                         1783 non-null   float64\n",
      " 1   D                         1783 non-null   float64\n",
      " 2   E                         1783 non-null   float64\n",
      " 3   CPI                       1783 non-null   float64\n",
      " 4   GS10                      1783 non-null   float64\n",
      " 5   Price                     1783 non-null   float64\n",
      " 6   Dividend                  1783 non-null   float64\n",
      " 7   Earnings                  1783 non-null   float64\n",
      " 8   pct_change                1783 non-null   float64\n",
      " 9   log_ret_1_mth             1783 non-null   float64\n",
      " 10  Log_Ret_2_mth             1783 non-null   float64\n",
      " 11  Log_Ret_3_mth             1783 non-null   float64\n",
      " 12  Log_Ret_4_mth             1783 non-null   float64\n",
      " 13  Log_Ret_8_mth             1783 non-null   float64\n",
      " 14  Log_Ret_12_mth            1783 non-null   float64\n",
      " 15  Log_Ret_48_mth            1783 non-null   float64\n",
      " 16  volat_2_mth               1783 non-null   float64\n",
      " 17  volat_3_mth               1783 non-null   float64\n",
      " 18  volat_4_mth               1783 non-null   float64\n",
      " 19  volat_8_mth               1783 non-null   float64\n",
      " 20  volat_12_mth              1783 non-null   float64\n",
      " 21  volat_48_mth              1783 non-null   float64\n",
      " 22  rolling_Earnings_mean12   1783 non-null   float64\n",
      " 23  rolling_Earnings_mean120  1783 non-null   float64\n",
      " 24  CAPE10                    1783 non-null   float64\n",
      " 25  dir                       1783 non-null   int32  \n",
      "dtypes: float64(25), int32(1)\n",
      "memory usage: 369.1 KB\n"
     ]
    }
   ],
   "source": [
    "# making sure we don't have missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dropping justified by high intercorrelation of Price and Aggregate Price, \n",
    "#Earnings vs Aggregate Earnings, etc. from the corr. matrix plot in 1st notebook\n",
    "df=df.drop(['Price','Dividend','Earnings'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1783 entries, 1871-01-01 to 2019-07-01\n",
      "Data columns (total 23 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   P                         1783 non-null   float64\n",
      " 1   D                         1783 non-null   float64\n",
      " 2   E                         1783 non-null   float64\n",
      " 3   CPI                       1783 non-null   float64\n",
      " 4   GS10                      1783 non-null   float64\n",
      " 5   pct_change                1783 non-null   float64\n",
      " 6   log_ret_1_mth             1783 non-null   float64\n",
      " 7   Log_Ret_2_mth             1783 non-null   float64\n",
      " 8   Log_Ret_3_mth             1783 non-null   float64\n",
      " 9   Log_Ret_4_mth             1783 non-null   float64\n",
      " 10  Log_Ret_8_mth             1783 non-null   float64\n",
      " 11  Log_Ret_12_mth            1783 non-null   float64\n",
      " 12  Log_Ret_48_mth            1783 non-null   float64\n",
      " 13  volat_2_mth               1783 non-null   float64\n",
      " 14  volat_3_mth               1783 non-null   float64\n",
      " 15  volat_4_mth               1783 non-null   float64\n",
      " 16  volat_8_mth               1783 non-null   float64\n",
      " 17  volat_12_mth              1783 non-null   float64\n",
      " 18  volat_48_mth              1783 non-null   float64\n",
      " 19  rolling_Earnings_mean12   1783 non-null   float64\n",
      " 20  rolling_Earnings_mean120  1783 non-null   float64\n",
      " 21  CAPE10                    1783 non-null   float64\n",
      " 22  dir                       1783 non-null   int32  \n",
      "dtypes: float64(22), int32(1)\n",
      "memory usage: 327.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train test split; note: shuffling not allowed due to nature of fin. time series\n",
    "X_train_1, X_test_1, y_train_1, y_test_1=train_test_split(df.iloc[:,:22], df.iloc[:,22:], test_size=0.1 ,shuffle=False, stratify=None)\n",
    "# \n",
    "# LSTM trials - Input arrays should be shaped as (samples or batch, time_steps or look_back, num_features):\n",
    "#X_test_1_lstm=X_test_1.values.reshape(X_test_1.shape[0], 1, X_test_1.shape[1])\n",
    "fitting_scaler = StandardScaler()\n",
    "\n",
    "X_train_1_tr = X_train_1.copy()\n",
    "X_train_1_tr = fitting_scaler.fit_transform(X_train_1_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to implement k-fold validation for different classifiers\n",
    "def kfold_model_run(model, X, y, K=10):\n",
    "    kfold = KFold(n_splits=K)\n",
    "    kfold.get_n_splits(X)\n",
    "    accuracy = np.zeros(K)\n",
    "    np_idx = 0\n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y.values[train_idx], y.values[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        ACC = accuracy_score(y_test, predictions)\n",
    "        accuracy[np_idx] = ACC*100\n",
    "        np_idx += 1\n",
    "\n",
    "        print (\"Fold {}: Accuracy: {}\".format(np_idx, round(ACC,3)))   \n",
    "\n",
    "    print (\"Average Score is: {}%({}%)\".format(round(np.mean(accuracy),3),round(np.std(accuracy),3)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the _logistic regression_ model with all the features included as a __baseline__ model.  Running the logistic regression model with all the features included as a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy: 0.988\n",
      "Fold 2: Accuracy: 0.988\n",
      "Fold 3: Accuracy: 0.988\n",
      "Fold 4: Accuracy: 0.994\n",
      "Fold 5: Accuracy: 0.975\n",
      "Fold 6: Accuracy: 0.994\n",
      "Fold 7: Accuracy: 1.0\n",
      "Fold 8: Accuracy: 0.994\n",
      "Fold 9: Accuracy: 0.981\n",
      "Fold 10: Accuracy: 0.931\n",
      "Average Score is: 98.315%(1.856%)\n",
      "Time taken to run this model is 0.21132636070251465\n",
      "Total number of features are: 22\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "start_time = time.time()\n",
    "lg = LogisticRegression()\n",
    "kfold_model_run(lg, X_train_1_tr, y_train_1)\n",
    "elapsedtime = time.time() - start_time\n",
    "print('Time taken to run this model is {}'.format(elapsedtime))\n",
    "print('Total number of features are: {}'.format(X_train_1_tr.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some importance feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy: 1.0\n",
      "Fold 2: Accuracy: 1.0\n",
      "Fold 3: Accuracy: 0.996\n",
      "Average Score is: 99.875%(0.177%)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# preraration to use XGBoost's important component analysis; \n",
    "\n",
    "\n",
    "g_boost = XGBClassifier()\n",
    "kfold_model_run(g_boost, X_train_1_tr, y_train_1, K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# building the importance plot\n",
    "ax = plot_importance(g_boost, color='red')\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(30, 15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train_1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature = g_boost.feature_importances_\n",
    "Dic = dict(zip(feature_names,feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features sorted in order are:\n",
      "pct_change\n"
     ]
    }
   ],
   "source": [
    "print('Top features sorted in order are:')\n",
    "\n",
    "print(feature_names[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the only important column that is claimed to provide most prediction accuracy is considered the Percentage Change. This is inline with the fact that many traders prefer percentage change when calculating rolling averages, [6]. Let's perform chi-square test to test variables' independence hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.44167204, 1.44028383, 1.16359532, 1.82963604, 0.01045091,\n",
       "        6.37187423, 5.94818359, 5.72357805, 3.76791223, 3.41097276,\n",
       "        3.00852125, 2.0627969 , 1.21128625, 0.43265664, 0.36732416,\n",
       "        0.30281962, 0.27843154, 0.07580658, 0.01269798, 0.93674661,\n",
       "        1.33445505, 0.31154151]),\n",
       " array([0.50631653, 0.23009342, 0.28072115, 0.17617054, 0.91857437,\n",
       "        0.01159431, 0.01473246, 0.01673859, 0.05224481, 0.06476426,\n",
       "        0.08282782, 0.15093292, 0.27107753, 0.51068825, 0.54446645,\n",
       "        0.58212015, 0.59773109, 0.78306276, 0.91028004, 0.33311594,\n",
       "        0.2480142 , 0.57673579]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "fitting_scaler = MinMaxScaler()\n",
    "X_train_1_tr1 = X_train_1.copy()\n",
    "X_train_1_tr1 = fitting_scaler.fit_transform(X_train_1_tr1)\n",
    "chi_scores = chi2(X_train_1_tr1,y_train_1)\n",
    "chi_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ca775a3dc8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_values = pd.Series(chi_scores[1],index = X_train_1.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)\n",
    "\n",
    "p_values.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis (H0): The variables are independent.\n",
    "Alternative Hypothesis (H1): The variables are not independent.\n",
    "Here we have 21 category predictors and one category response (direction 'up' or 'down')\n",
    "Since 'GS10',etc. have highest the p-value, it says that these variables are independent of the response and better not be considered for model training. Before dropring some of them it's better to consider some hyperparameter tuning. The importance of column 'pct_change' is justified again. But the CAPE ratio does not seem to have strong predictive power. Let's run the baseline model on a reduced set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting top 13 features\n",
      "Fold 1: Accuracy: 0.901\n",
      "Fold 2: Accuracy: 0.913\n",
      "Fold 3: Accuracy: 0.919\n",
      "Fold 4: Accuracy: 0.925\n",
      "Fold 5: Accuracy: 0.888\n",
      "Fold 6: Accuracy: 0.894\n",
      "Fold 7: Accuracy: 0.881\n",
      "Fold 8: Accuracy: 0.881\n",
      "Fold 9: Accuracy: 0.875\n",
      "Fold 10: Accuracy: 0.825\n",
      "Average Score is: 89.021%(2.717%)\n",
      "Time taken to run this model is 0.14064884185791016 sec\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "print('Selecting top 13 features')\n",
    "X_train_cp = X_train_1.copy()\n",
    "X_train_less = X_train_cp[['P','rolling_Earnings_mean12','E','Log_Ret_48_mth','rolling_Earnings_mean120',\n",
    "                           'D','CPI','Log_Ret_12_mth','Log_Ret_8_mth','Log_Ret_4_mth','Log_Ret_3_mth', \n",
    "                           'Log_Ret_2_mth','log_ret_1_mth','pct_change']]\n",
    "# scale features \n",
    "X_train_scale = fitting_scaler.fit_transform(X_train_less)\n",
    "\n",
    "start_time = time.time()\n",
    "kfold_model_run(lg, X_train_scale, y_train_1)\n",
    "elapsedtime = time.time() - start_time\n",
    "print('Time taken to run this model is {} sec'.format(elapsedtime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe an average score worsening for about 9%. This could be due to some hidden interconnections between variables as well as not the best set of hyperparameters selected. Also, it is said that the k-fold does not work well for financial time series ,Lpez de Prado [9] , and that walk-forward-cross-validation is considered better (this would be left as an area for future research). For now we'd try to perform hyperparameter tuning and to play on safe side, we'd use the set parameters which performed best on baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model implementation\n",
    "In this section well implement five different classification algorithms, namely: Logistic Regression, Random Forest, KNN, Gradient boosting and Decision Tree to be performed on the training dataset using 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the features \n",
    "X_train_1_cp2 = X_train_1.copy()\n",
    "X_train = fitting_scaler.fit_transform(X_train_1_cp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_model = LogisticRegression()\n",
    "Random_Forest_model = RandomForestClassifier()\n",
    "knn_model = KNeighborsClassifier(n_neighbors=20)\n",
    "g_boost = XGBClassifier()\n",
    "dtree_model = tree.DecisionTreeClassifier(criterion = \"entropy\",min_samples_leaf = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 10-fold validation on Logistic Regression\n",
      "Fold 1: Accuracy: 0.913\n",
      "Fold 2: Accuracy: 0.938\n",
      "Fold 3: Accuracy: 0.913\n",
      "Fold 4: Accuracy: 0.919\n",
      "Fold 5: Accuracy: 0.881\n",
      "Fold 6: Accuracy: 0.875\n",
      "Fold 7: Accuracy: 0.881\n",
      "Fold 8: Accuracy: 0.894\n",
      "Fold 9: Accuracy: 0.862\n",
      "Fold 10: Accuracy: 0.856\n",
      "Average Score is: 89.332%(2.527%)\n",
      "Running 10-fold validation on Decision Tree\n",
      "Fold 1: Accuracy: 1.0\n",
      "Fold 2: Accuracy: 1.0\n",
      "Fold 3: Accuracy: 1.0\n",
      "Fold 4: Accuracy: 1.0\n",
      "Fold 5: Accuracy: 1.0\n",
      "Fold 6: Accuracy: 1.0\n",
      "Fold 7: Accuracy: 1.0\n",
      "Fold 8: Accuracy: 1.0\n",
      "Fold 9: Accuracy: 1.0\n",
      "Fold 10: Accuracy: 0.994\n",
      "Average Score is: 99.938%(0.188%)\n",
      "Running 10-fold validation on Random Forest\n",
      "Fold 1: Accuracy: 1.0\n",
      "Fold 2: Accuracy: 1.0\n",
      "Fold 3: Accuracy: 1.0\n",
      "Fold 4: Accuracy: 1.0\n",
      "Fold 5: Accuracy: 1.0\n",
      "Fold 6: Accuracy: 1.0\n",
      "Fold 7: Accuracy: 1.0\n",
      "Fold 8: Accuracy: 1.0\n",
      "Fold 9: Accuracy: 1.0\n",
      "Fold 10: Accuracy: 0.994\n",
      "Average Score is: 99.938%(0.188%)\n",
      "Running 10-fold validation on KNN, K = 20\n",
      "Fold 1: Accuracy: 0.826\n",
      "Fold 2: Accuracy: 0.87\n",
      "Fold 3: Accuracy: 0.851\n",
      "Fold 4: Accuracy: 0.863\n",
      "Fold 5: Accuracy: 0.85\n",
      "Fold 6: Accuracy: 0.888\n",
      "Fold 7: Accuracy: 0.819\n",
      "Fold 8: Accuracy: 0.781\n",
      "Fold 9: Accuracy: 0.712\n",
      "Fold 10: Accuracy: 0.656\n",
      "Average Score is: 81.162%(7.068%)\n",
      "Running 10-fold validation on Gradient Boosting\n",
      "Fold 1: Accuracy: 1.0\n",
      "Fold 2: Accuracy: 1.0\n",
      "Fold 3: Accuracy: 1.0\n",
      "Fold 4: Accuracy: 1.0\n",
      "Fold 5: Accuracy: 1.0\n",
      "Fold 6: Accuracy: 1.0\n",
      "Fold 7: Accuracy: 1.0\n",
      "Fold 8: Accuracy: 1.0\n",
      "Fold 9: Accuracy: 1.0\n",
      "Fold 10: Accuracy: 0.994\n",
      "Average Score is: 99.938%(0.188%)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# fit models with training data.\n",
    "print('Running 10-fold validation on Logistic Regression')\n",
    "kfold_model_run(logistic_reg_model, X_train, y_train_1)\n",
    "print('Running 10-fold validation on Decision Tree')\n",
    "dt = kfold_model_run(dtree_model, X_train, y_train_1)\n",
    "print('Running 10-fold validation on Random Forest')\n",
    "kfold_model_run(Random_Forest_model, X_train, y_train_1)\n",
    "print('Running 10-fold validation on KNN, K = 20')\n",
    "kfold_model_run(knn_model, X_train, y_train_1)\n",
    "print('Running 10-fold validation on Gradient Boosting')\n",
    "kfold_model_run(g_boost, X_train, y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression accuracy somehow deteriorated, decision tree, random forest and XGBoost shown good results. But we should not be too excited since decision trees are prone to overfitting and we must see the overall performance on out-of-sample test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Model Tuning \n",
    "Goals: \n",
    " - Improve the performance of the models from the previous step and select a final optimal model using grid search (parameter sweep) based on a metric (or metrics) choosen. \n",
    "- Selecting performance measures, such as accuracy, true positive rate (TPR), false positive rate (FPR), etc, to compare the model performance and explain why the selected criteria was choosen for best model \n",
    "\n",
    "from http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "We are chosing GridSearchCV, a method for tuning the hyperparameters that would result in optimized model selection.\n",
    "\n",
    "Model selection and evaluation using tools, such as model_selection.GridSearchCV and model_selection.cross_val_score, take a scoring parameter that controls what metric they apply to the estimators evaluated.\n",
    "\n",
    "Grid search is building several models having all the combinations of parameter in place,. It then runs a defult of three cross validations to return a set of parameters which proved to have highest accuracy score on the validation set based on the chosen accuracy scorer.\n",
    "\n",
    "GridSearchCV automatically refits the best model using all of the data that best fitted model is stored in grid object \n",
    "we can then use prediction using the best fitted model\n",
    "\n",
    "A search consists of:\n",
    "-an estimator (regressor or classifier such as RandomForestClassifier(), or LogisticRegression())\n",
    "-a parameter space\n",
    "-a method for searching or sampling candidates\n",
    "-a cross-validation scheme\n",
    "-a score function, such as accurracy_score()\n",
    "\n",
    "We used The accuracy_score fuction of defult library to compare the scores across  different hyper-parameters. \n",
    "The optimal hyperparameter set is then chosen to run 10-fold cross validation in order to obtain the average score of optimal parameters. It was also used to get the confusion matrix and run-time of all the implemented algorithms. \n",
    "\n",
    "On a separate note - other techniques like Random search and more advanced Baesian frameworks are used generally. The latter proven to be one of the most efficient approaches for search in hyperparameter space.\n",
    "\n",
    "Finally, we selected the model with highest score which has proven to be the Gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression model hyper-parameter tuning\n",
    "lg = LogisticRegression(random_state=42)\n",
    "# Creating hyperparameter search space  and regularization penalty space\n",
    "\n",
    "penalty = ['l1', 'l2']\n",
    "# Creating regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Creating hyperparameter options\n",
    "lg_params = dict(C=C, penalty=penalty)\n",
    "\n",
    "### Decision Tree model hyper-parameter tuning\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Choosing some parameter combinations to tune\n",
    "dt_params = {\"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"min_samples_split\": [2, 10, 20],\n",
    "              \"max_depth\": [None, 2, 5, 10],\n",
    "              \"min_samples_leaf\": [1, 5, 10, 100,500, 1000],\n",
    "              \"max_leaf_nodes\": [None, 5, 10, 20,25],\n",
    "              }\n",
    "\n",
    "### Random Forest model hyper-parameter tuning\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Choosing some parameter combinations to tune\n",
    "rf_params = {'n_estimators': [4, 9, 15], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 5, 10,15], \n",
    "              'min_samples_split': [2, 3, 5,10],\n",
    "              'min_samples_leaf': [1,5,8]\n",
    "             }\n",
    "\n",
    "### KNN model hyper-parameter tuning\n",
    "knn = KNeighborsClassifier()\n",
    "# Choosing some parameter combinations to tune\n",
    "knn_params = {\"n_neighbors\": np.arange(5, 35, 4), \n",
    "              \"metric\": [\"euclidean\", 'minkowski']}\n",
    "\n",
    "### Gradient boost model hyper-parameter tuning\n",
    "gboost = XGBClassifier()\n",
    "\n",
    "# Choosing some parameter combinations to tune\n",
    "gboost_params = {'gamma': [0.05, 0.1, 0.3, 0.5,1], \n",
    "              'max_depth': [3, 5, 9], \n",
    "              'subsample': [0.6, 0.8, 0.9, 1]\n",
    "                }\n",
    "\n",
    "def run_gridsearch(clf, X, y, hyperparameters, cv=5):\n",
    "    \n",
    "    # Using classification accuracy to compare parameter combinations\n",
    "    acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "\n",
    "    # Creating grid search using k-fold cross-validation\n",
    "    grid_search = GridSearchCV(clf, hyperparameters, cv=cv, scoring=acc_scorer)\n",
    "    grid_search = grid_search.fit(X, y)\n",
    "    \n",
    "    print('Best Score is:')\n",
    "    best_score = grid_search.best_score_ \n",
    "    print((best_score*100))\n",
    "    print('Best Model Hyperparameters are:')\n",
    "    print(grid_search.best_params_)\n",
    "    print('Best Model fit is:')\n",
    "    print(grid_search.best_estimator_)\n",
    "    \n",
    "    # Setting classifier to utilize the best parameters combination\n",
    "    best_clf = grid_search.best_estimator_\n",
    "     \n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Tuning\n",
      "Best Score is:\n",
      "98.31619937694704\n",
      "Best Model Hyperparameters are:\n",
      "{'C': 10000.0, 'penalty': 'l2'}\n",
      "Best Model fit is:\n",
      "LogisticRegression(C=10000.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Decision Tree Tuning\n",
      "Best Score is:\n",
      "99.875\n",
      "Best Model Hyperparameters are:\n",
      "{'criterion': 'gini', 'max_depth': None, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Model fit is:\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "Random Forest Tuning\n",
      "Best Score is:\n",
      "99.9375\n",
      "Best Model Hyperparameters are:\n",
      "{'criterion': 'entropy', 'max_depth': 2, 'max_features': 'log2', 'min_samples_leaf': 8, 'min_samples_split': 3, 'n_estimators': 4}\n",
      "Best Model fit is:\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=2, max_features='log2',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=8, min_samples_split=3,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=4,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "KNN Tuning\n",
      "Best Score is:\n",
      "63.150311526479754\n",
      "Best Model Hyperparameters are:\n",
      "{'metric': 'euclidean', 'n_neighbors': 33}\n",
      "Best Model fit is:\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=33, p=2,\n",
      "                     weights='uniform')\n",
      "Gradient Boosting Tuning\n",
      "Best Score is:\n",
      "99.875\n",
      "Best Model Hyperparameters are:\n",
      "{'gamma': 0.05, 'max_depth': 3, 'subsample': 0.6}\n",
      "Best Model fit is:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0.05, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, subsample=0.6,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "print('Logistic Regression Tuning')\n",
    "best_lg = run_gridsearch(lg, X_train, y_train_1, lg_params)\n",
    "print('Decision Tree Tuning')\n",
    "best_dt = run_gridsearch(dt, X_train, y_train_1, dt_params)\n",
    "print('Random Forest Tuning')\n",
    "best_rf = run_gridsearch(rf, X_train, y_train_1, rf_params)\n",
    "print('KNN Tuning')\n",
    "best_knn = run_gridsearch(knn, X_train, y_train_1, knn_params)\n",
    "print('Gradient Boosting Tuning')\n",
    "best_gboost = run_gridsearch(gboost, X_train, y_train_1, gboost_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was envisioned as a possible outcome - Logistic regression's performance is good again on full set of parameters. This is true even when comparing logistic regression results on 10-fold validation for Logistic Regression (88% then vs 98.3% now after tuning). However, the absolute winner seems to be XGboost with 99.875% prediction accuracy on training set. Random forests demonstrated same accuracy (on some runs it is even higher like in case of last run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are performing the evaluation of the best performing model once the hyperparameter tuning is made\n",
    "\n",
    "def model_eval(best_clf, X, y, K=5):\n",
    "    # Fitting the best model with optimal hyperparameters to the training data. \n",
    "    kfold = KFold(n_splits=K)\n",
    "    kfold.get_n_splits(X)\n",
    "    \n",
    "    # calculating the accuracy below\n",
    "    accuracy = np.zeros(K)\n",
    "    np_idx = 0\n",
    "    best_acc = 0\n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y.values[train_idx], y.values[test_idx]\n",
    "\n",
    "        best_clf.fit(X_train, y_train)\n",
    "\n",
    "        predictions = best_clf.predict(X_test)\n",
    "\n",
    "        ACC = accuracy_score(y_test, predictions)\n",
    "        accuracy[np_idx] = ACC*100\n",
    "        np_idx += 1\n",
    "\n",
    "          \n",
    "        if ACC > best_acc:\n",
    "            best_acc = ACC\n",
    "            perform_measure = confusion_matrix(y_test, predictions)\n",
    "    print(\"Model {}\".format(best_clf))\n",
    "    print (\"The average score is: {}%({}%)\".format(round(np.mean(accuracy),3),\n",
    "                                            round(np.std(accuracy),3)))\n",
    "    print ('The confussion matrix is:')\n",
    "    print(perform_measure)\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Model LogisticRegression(C=10000.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "The average score is: 98.066%(1.347%)\n",
      "The confussion matrix is:\n",
      "[[130   0]\n",
      " [  1 190]]\n",
      "Time taken to run Logistic Regression model is 0.17057228088378906 sec\n",
      "Random Forest\n",
      "Model RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=2, max_features='log2',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=8, min_samples_split=3,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=4,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "The average score is: 97.507%(2.814%)\n",
      "The confussion matrix is:\n",
      "[[175   0]\n",
      " [  0 146]]\n",
      "Time taken to run Random Forest model is 0.05984067916870117 sec\n",
      "K-NN\n",
      "Model KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=33, p=2,\n",
      "                     weights='uniform')\n",
      "The average score is: 82.913%(4.489%)\n",
      "The confussion matrix is:\n",
      "[[ 99  30]\n",
      " [  9 183]]\n",
      "Time taken to run KNN model is 0.17353606224060059 sec\n",
      "Gradient Boosting\n",
      "Model XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "The average score is: 99.875%(0.25%)\n",
      "The confussion matrix is:\n",
      "[[175   0]\n",
      " [  0 146]]\n",
      "Time taken to run Gradient Boosting model is 0.33709716796875 sec\n",
      "Decision Tree\n",
      "Model DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "The average score is: 99.875%(0.25%)\n",
      "The confussion matrix is:\n",
      "[[175   0]\n",
      " [  0 146]]\n",
      "Time taken to run Decision Tree model is 0.030916929244995117 sec\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "print('Logistic Regression')\n",
    "start_time = time.time()\n",
    "model_eval(best_lg, X_train, y_train_1)\n",
    "elapsedtime = time.time() - start_time\n",
    "print('Time taken to run Logistic Regression model is {} sec'.format(elapsedtime))\n",
    "\n",
    "print('Random Forest')\n",
    "start_time = time.time()\n",
    "model_eval(best_rf, X_train, y_train_1)\n",
    "elapsedtime = time.time() - start_time\n",
    "print('Time taken to run Random Forest model is {} sec'.format(elapsedtime))\n",
    "\n",
    "print('K-NN')\n",
    "start_time = time.time()\n",
    "model_eval(best_knn, X_train, y_train_1)\n",
    "elapsedtime = time.time() - start_time\n",
    "print('Time taken to run KNN model is {} sec'.format(elapsedtime))\n",
    "\n",
    "\n",
    "print('Gradient Boosting')\n",
    "start_time = time.time()\n",
    "model_eval(g_boost, X_train, y_train_1)\n",
    "elapsedtime = time.time() - start_time\n",
    "print('Time taken to run Gradient Boosting model is {} sec'.format(elapsedtime))\n",
    "\n",
    "print('Decision Tree')\n",
    "start_time = time.time()\n",
    "model_eval(best_dt, X_train, y_train_1)\n",
    "elapsedtime = time.time() - start_time\n",
    "print('Time taken to run Decision Tree model is {} sec'.format(elapsedtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of best performing Model\n",
    "\n",
    "We will try to make a prediciton test dataset for market direction prediction using the most optimal XGBoost model implemented above\n",
    "\n",
    "We must also answer the question: how the model performs on the test data vs. the train data?\n",
    "The overall fit of the model, how to increase the accuracy (test, training)? Is it overfitting or underfitting\n",
    "\n",
    "In order to find answers to these questions we've tried to compare the training and testing accuracy\n",
    "and found out that the latter is comparable to the training accuracy. Hence, we would be able to conclude whether the model behaves well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# obtaining the most optimal model\n",
    "optimal_model =  best_gboost\n",
    "X_test_1_cp = X_test_1.copy()\n",
    "\n",
    "# scaling all the features \n",
    "X_test = fitting_scaler.fit_transform(X_test_1_cp)\n",
    "\n",
    "# training the optimal model on the entire training dataset \n",
    "optimal_model.fit(X_train, y_train_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the testing dataset's 'dir' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracy Score of the Optimal Model is: 68.0%\n",
      "The Confussion Matrix is:\n",
      "[[  2  58]\n",
      " [  0 119]]\n"
     ]
    }
   ],
   "source": [
    "predictions = optimal_model.predict(X_test)\n",
    "ACC = accuracy_score(y_test_1, predictions)\n",
    "perform_measure = confusion_matrix(y_test_1, predictions)\n",
    "\n",
    "print(\"Acuracy Score of the Optimal Model is: {}%\".format(round(ACC*100),1))\n",
    "print ('The Confussion Matrix is:')\n",
    "print(perform_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of XGBoost dropped to 68.0% on unseen data which is not too bad, but it should be investigated how to improve this result. One of the reasons is the higher imbalance of 'Ups' vs 'Downs' in testing dataset as one can see from results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further thoughts on possible imbalance of sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5579800498753117"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_ups = (y_train_1['dir'] == 1).sum()\n",
    "print(y_train_ups)\n",
    "\n",
    "y_trn_ups_pct = y_train_ups / y_train_1['dir'].count().astype(float)\n",
    "y_trn_ups_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.664804469273743"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_ups = (y_test_1['dir'] == 1).sum()\n",
    "print(y_test_ups)\n",
    "\n",
    "y_tst_ups_pct = y_test_ups / y_test_1['dir'].count().astype(float)\n",
    "y_tst_ups_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training set the distribution is roughly 56%-44% in favor of Ups, and for the test set it is 66%-34% also in favor of Ups. Different techniques are in place to do resamplings for imbalanced classes , e.g. SMOTE or recently GAN's. But our imbalance is not really huge and also the GANs is seemingly a heavy machinery which is out of scope of this mini-evaluation. We'd conentrate more on AUC (area under ROC curve), which is said to be not sensitive to imbalances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "This model does not account for inflation - consistently defined and longterm running series on U.S. inflation since the 1920s does not exist and inflation expectations could be modelled as the average of the predicted CPI inflation rate over the next 10 years generated from an AR model at any month in time [10].\n",
    "Model's behaviour in case of negative interest rates remains a topic for investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future area of research\n",
    "Another area of future research could be using partial differencing,  because using standard differencing results in loss of memory for the sake of achieving stationarity, Lpez de Prado[8]\n",
    "\n",
    "As another state-of-the-art we select LSTM method mainly due to its property to  presere and train the features  of  given  data for  larger timeframes. A notable mention is bidirectional LSTM (BiLSTM) version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] _Online Data Robert Shiller_ , weblink: http://www.econ.yale.edu/~shiller/data.htm\n",
    "\n",
    "[2] Robert J. Shiller, _Irrational Exuberance_ [Princeton University Press 2000, Broadway Books 2001, 2nd ed., 2005] \n",
    "\n",
    "[3] Xiaoyue Wang, Abdullah Mueen, Hui Ding, Goce Trajcevski, Peter Scheuermann, and Eamonn Keogh. _Experimental comparison of representation methods and distance measures for time series data_. Data Mining and Knowledge Discovery, 26(2):275309, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] Merton H. Miller and Kevin Rock, _Dividend Policy under Asymmetric Information_ The Journal of Finance\n",
    "Vol. 40, No. 4 (Sep., 1985), pp. 1031-1051"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] Stephen Marra,  _Predicting volatility_ , Lazard Asset Management, weblink: \n",
    "https://www.lazardassetmanagement.com/docs/-m0-/22430/predictingvolatility_lazardresearch_en.pdf\n",
    "\n",
    "[6] _Price Rate Of Change Indicator (ROC)_ , weblink: https://www.investopedia.com/terms/p/pricerateofchange.asp\n",
    "\n",
    "[7] Chen, K., Zhou, Y., & Dai, F. (2015), _A LSTM-based method for stock returns\n",
    "prediction: A case study of china stock market_ , in Proceedings of the 2015 IEEE\n",
    "International Conference on Big Data, pp. 28232824.\n",
    "\n",
    "[8] Lpez de Prado, Marcos, _Ten Financial Applications of Machine Learning (Seminar Slides)_ (June 16, 2018). Available at SSRN: https://ssrn.com/abstract=3197726 or http://dx.doi.org/10.2139/ssrn.3197726\n",
    "\n",
    "[9] Lpez de Prado, M. (2018), _Advances in Financial Machine Learning_ , New Jersey 2018\n",
    "\n",
    "[10] Wang, Haifeng and Ahluwalia, Harshdeep and Aliaga-Diaz, Roger A and Davis, Joseph H., _The Best of Both Worlds: Forecasting US Equity Market Returns using a Hybrid Machine Learning  Time Series Approach_ (December 2, 2019). Available at SSRN: https://ssrn.com/abstract=3497170 or http://dx.doi.org/10.2139/ssrn.3497170\n",
    "\n",
    "[11] R. Nau, _Statistical Forecasting_ weblink: a) https://people.duke.edu/~rnau/Notes_on_nonseasonal_ARIMA_models--Robert_Nau.pdf , \n",
    "\n",
    "b) https://people.duke.edu/~rnau/Slides_on_ARIMA_models--Robert_Nau.pdf\n",
    "\n",
    "[12] weblink: https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
